# LLM Configuration
# Choose your LLM provider: "openai" or "ollama"
LLM_PROVIDER=ollama

# Model name (optional - defaults based on provider)
# For OpenAI: gpt-4, gpt-4o, gpt-4o-mini, gpt-3.5-turbo, etc.
# For Ollama: llama3.2, llama3.1, mistral, codellama, etc.
LLM_MODEL=llama3.2

# Temperature for generation (0.0 to 1.0)
LLM_TEMPERATURE=0.7

# OpenAI Configuration (only needed if LLM_PROVIDER=openai)
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your-openai-api-key-here

# Ollama Configuration (only needed if LLM_PROVIDER=ollama)
# Default: http://localhost:11434
# Change this if Ollama is running on a different host/port
OLLAMA_BASE_URL=http://localhost:11434
